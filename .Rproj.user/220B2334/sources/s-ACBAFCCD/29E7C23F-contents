#librerías generales
library(dplyr)
library(ggplot2)
library(quanteda)


##Paquete tweetR

#install.packages("twitterR")
library(twitteR)



# Para hacer la conexión con la api de twitter
# Keys and Token
api_key <- "9cEVJjdGIuKjaL7xYqkdhD1LI"
api_secret <- "sHBJLc1Iwpgdy9yjSls3X9DkyPfJciNxQASbB44auhgItO6GKA"
access_token <- "1140008627014635520-vY2FXYlCrH4yIeMADX1r1lelngwzWn" #
access_token_secret<- "94KCMQXD3tppzQIp4q0hhOTn9PX4r4563Q1ZT5mpl2gwn" #
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)

# 
# #obtengo los tweets
# # 2000 tweets más recientes
# tweets_fa = searchTwitter("frente amplio OR frenteamplio", n = 10, lang = "es")
# ?searchTwitteR
# 
# 1#creo un data.frame
# tweets_fa_df = twListToDF(tweets_fa)
# 
# ##los transformo en un corpus de datos
# # se crea un corpus con la variable text
# tweets_fa <- Corpus(
#   VectorSource(
#     tweets_fa_df$text),
#   readerControl = list(encoding = "UTF-8",
#                        language ="es"))

# EJERCICIO

# 1. Descargar 2000 tweets para dos candidatos presidenciales
tweets_cosse <- searchTwitter("Cosse OR Carolina Cosse OR CosseCarolina",
                              n = 10000, lang = "es")
tweets_larranaga <- searchTwitter("Larrañaga OR Jorge Larrañaga OR jorgewlarranaga",
                                n = 10000, lang = "es")

# 2. Se unen las dos bases y se agrega la variable "nombre", con los nombres de los candidatos
tweets_candidatos_df <- twListToDF(tweets_cosse) %>% 
  mutate(candidato = "cosse") %>% 
  rbind(twListToDF(tweets_larranaga) %>% 
          mutate(candidato = "larranaga"))

# 3. Limpiar el texto con el paquete quanteda y construir un DFM
# Se genera el corpus
tweets_candidatos <- quanteda::corpus(tweets_candidatos_df, text_field = "text")

# Abro un archivo con stopwords propias y modismos
vector <- file.path('.', 'data', 'stopes.csv') %>% 
  read.csv(sep = ";", 
           encoding = "utf-8",
           stringsAsFactors = FALSE) %>% 
  transmute(stop = as.character(X0))

# Aplico la función dfm con los argumentos para limpiar el texto
mydfm <- dfm(tweets_candidatos,
             stem = FALSE,
             tolower = TRUE,                                         # paso a minúscula todas las palabras
             remove = c(stopwords("spanish"), vector),               # saco las palabras definidas en stop
             remove_punct = TRUE,                                    # elimino puntuaciones
             remove_numbers = TRUE,                                  # elimino números
             verbose = TRUE)

# Me quedo sólo con las palabras
palabras <- featnames(mydfm)

# Saco palabras con 1 y 2 caracteres
palabras_tam_1 <- palabras[sapply(palabras, stringr::str_length) == 1]
palabras_tam_2 <- palabras[sapply(palabras, stringr::str_length) == 2]

# # uno el vector con otra/s palabra/s
# otraspalabras <- c("risas", 
#                    "@cossecarolina", 
#                    "carolina",
#                    "cosse",
#                    "carolina cosse",
#                    "juan",
#                    "sartori",
#                    "juan sartori",
#                    "sartori",
#                    "@juansartoriuy",
#                    palabras_tam_1, 
#                    palabras_tam_2)          # todas las palabras a eliminar
# 
# # lo renuevo
# mydfm <- dfm(mydfm, remove = otraspalabras)

# 4. Hacer un worldcloud con los principales 400 términos, desagregado para cada candidato

# Nubes de palabras con quanteda
## Nubes de palabras sin desagregación
textplot_wordcloud(mydfm,
                   min.count = 10,
                   max_words = 400,
                   random.order = FALSE,
                   rot.per = .25, 
                   colors = RColorBrewer::brewer.pal(8,"Dark2"))

## Nubes de palabras por grupos

# genero un nuevo data.frame por grupos según alguna variable: sexo
dfm_candidato <- dfm(mydfm, groups = "candidato")

# tiro la función para generar nube de palabras 
textplot_wordcloud(dfm_candidato, 
                   min.count = 10,
                   max_words = 400,
                   random.order = FALSE,
                   rot.per = .25, 
                   colors = RColorBrewer::brewer.pal(8,"Dark2"),
                   comparison = T)

# 
# ##Paquete rtweet
# # instalar  el paquete
# # install.packages("rtweet")
# 
# #Otra alternativa de instalación directo de GITHUB:
# # if (!requireNamespace("remotes", quietly = TRUE)) {
# #   install.packages("remotes")
# # }
# # 
# # ## install dev version of rtweet from github
# # remotes::install_github("mkearney/rtweet")
# 
# ## load rtweet package
# library(rtweet)
# 
# ##con las claves de la app que creamos, cargamos las credenciales para conectarnos
# create_token(app = "R_cuali", 
#              consumer_key="9cEVJjdGIuKjaL7xYqkdhD1LI", 
#              consumer_secret="sHBJLc1Iwpgdy9yjSls3X9DkyPfJciNxQASbB44auhgItO6GKA",
#              access_token = "1140008627014635520-vY2FXYlCrH4yIeMADX1r1lelngwzWn", 
#              access_secret = "94KCMQXD3tppzQIp4q0hhOTn9PX4r4563Q1ZT5mpl2gwn", 
#              set_renv = TRUE)
# 
# # buscamos los tweets que mencionan un determinado usuarix o hashtag 
# # Por defecto considera los últimos 8 días
# fa <- search_tweets("@Frente_amplio", 
#                     n = 18000, 
#                     include_rts = FALSE)
# tu <- search_tweets("@transforma_uy", 
#                     n = 18000, 
#                     include_rts = TRUE)
# # ,
# #                     retryonratelimit = TRUE) # para extender límites de búsqueda
# 
# ## Graficamos usando la función ts_plot para series de tiempo en twitter
# 
# fa %>%
#   ts_plot("5 hours") +                                                 #Cada 5 horas
#   ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
#   ggplot2::labs(
#     x = NULL, y = NULL,
#     title = "Frecuencia de @Frente_Amplio de los últimos 9 días",
#     caption = "Recuento de tweets en intervalos de 5 horas"
#   )
# 
# tu %>%
#   ts_plot("5 hours") +                                                 #Cada 5 horas
#   ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
#   ggplot2::labs(
#     x = NULL, y = NULL,
#     title = "Frecuencia de @transforma_uy de los últimos 9 días",
#     caption = "Recuento de tweets en intervalos de 5 horas"
#   )
# 
# 
# ## Utilizamos las funciones get_followers() para obtener lxs usuarixs y 
# # lookup_users para obtener información de lxs mismxs
# 
# ##Análisis por candidatx
# 
# #@Dmartinez_uy
# dmart_flw <- get_followers("Dmartinez_uy", 
#                            n = "all")
# # dmart_flw <- get_followers("Dmartinez_uy", 
# #                            n = "all",
# #                            retryonratelimit=TRUE) # para no limitar a 75k seguidores
# 
# dmart_flw_data <- lookup_users(dmart_flw$user_id)
# # save(dmart_flw_data,file="Dmartinez_uy.RData")
# 
# #@CosseCarolina
# cc_flw <- get_followers("CosseCarolina", n = "all")
# cc_flw_data <- lookup_users(cc_flw$user_id)
# # save(cc_flw_data,file="CosseCarolina.RData")
# 
# #@LuisLacallePou
# llp_flw <- get_followers("LuisLacallePou", n = "all")
# llp_flw_data <- lookup_users(llp_flw$user_id)
# # save(llp_flw_data,file="LuisLacallePou.RData")
# 
# # #@JuanSartoriUY
# js_flw <- get_followers("JuanSartoriUY", n = "all")
# js_flw_data <- lookup_users(js_flw$user_id)
# # save(js_flw_data,file="JuanSartoriUY.RData")
# 
# #@ernesto_talvi
# et_flw <- get_followers("ernesto_talvi", n = "all")
# et_flw_data <- lookup_users(et_flw$user_id)
# # save(et_flw_data,file="ernesto_talvi.RData")
# 
# #genero una base por candidatx, agrego una variable con el nombre 
# #y una de conteo antes de unirlas
# 
# 
# dmart_flw_data <- dmart_flw_data %>%
#   mutate(candi="Daniel Martinez",
#          dmart_c=1)
# 
# llp_flw_data <- llp_flw_data %>%
#   mutate(candi="Lacalle Pou",
#          llp_c=1)
# 
# cc_flw_data <- cc_flw_data %>%
#   mutate(candi="Carolina Cosse",
#          cc_c=1)
# 
# js_flw_data  <- js_flw_data %>%
#   mutate(candi="Juan Sartori",
#          js_c=1)
# et_flw_data <- et_flw_data %>%
#   mutate(candi="Ernesto Talvi",
#          et_c=1)
# 
# 
# #Pongo todos los data.frames de lxs candidatxs en una lista
# bases <- list(dmart_flw_data, llp_flw_data, cc_flw_data, js_flw_data, et_flw_data)
# 
# #Uno todas las bases y selecciono las variables que me interesan                                    
# seguidores<-bind_rows(bases) %>%
#   select(user_id, text ,name, screen_name,country, country_code,geo_coords,coords_coords,
#          location, followers_count, statuses_count, account_created_at,candi,
#          ends_with('_c'))
# 
# seguidores[is.na(seguidores)] = 0
# 
# # save(seguidores,file="seguidores.RData")
# 
# 
# ##grafico la cantidad de seguidores por cada candidatx
# 
# 
# library(RColorBrewer)
# require(forcats)
# 
# # Conteo, cantidad de seguidores por candidato
# ggplot(seguidores, 
#        aes(fct_infreq(candi)))+ 
#   geom_bar(fill= brewer.pal(5,"Set2")) +
#   geom_text(stat='count', 
#             aes(label=..count..), 
#             vjust=-1)+
#   xlab("Candidatos") +
#   ylab("Cantidad de seguidorxs")
# 
# 
# 
# ##Intersecciones:
# 
# ##armo una base uniendo por user_id (saco duplicados)
# 
# # Unimos todas las bases                                    
# seguidores_join <- plyr::join_all(bases,
#                                   by = c("user_id"), 
#                                   type = "full")
# 
# seguidores_join <- seguidores_join %>%
#   select(user_id, text ,name, screen_name,country, country_code,geo_coords,coords_coords,
#          location, followers_count, statuses_count, account_created_at,candi,
#          ends_with('_c'))
# 
# #Reemplazamos blancos con cero
# seguidores_join[is.na(seguidores_join)] <- 0
# 
# #armo una variable que suma los conteos de cada candidatx
# seguidores_join <- seguidores_join %>%
#   mutate(cantidad = (dmart_c + llp_c + cc_c + js_c + et_c))
# 
# #save(seguidores_join,file="R_cuali/Material/Bases_clase6/seguidores_join.RData")
# 
# library(gmodels)
# 
# ## veo los cantidatxs comunes
# CrossTable(seguidores_join$cantidad, prop.t=F, prop.chisq = F)
# 
# 
# # Obtengo los n estados favoritos más recientes de un usuario 
# dm_fav <- get_favorites("Dmartinez_uy", n = 2000)
# 
# ## Uso y gráfico de get_timelines
# 
# serie <- get_timelines(c("Dmartinez_uy", "LuisLacallePou","JuanSartoriUY"), n = 3200)
# 
# serie %>%
#   dplyr::filter(created_at > "2019-05-01") %>%
#   dplyr::group_by(screen_name) %>%
#   ts_plot("days", trim = 1L) +
#   ggplot2::geom_point() +
#   ggplot2::theme_minimal() +
#   ggplot2::theme(
#     legend.title = ggplot2::element_blank(),
#     legend.position = "bottom",
#     plot.title = ggplot2::element_text(face = "bold")) +
#   ggplot2::labs(
#     x = NULL, y = NULL,
#     title = "Frecuencia de los estados de Twitter publicados por cada candidato",
#     subtitle = "Recuento de estados de Twitter agregados por día - Mayo/Junio 2019"
#   )
# 
# 
# 
# # EJERCICIO
# # 1. Tomo los 3000 estados favoritos de Juan Sartori y convierto el texto en un Corpus
# js_fav <- get_favorites("JuanSartoriUY", n = 3000) %>% 
#   quanteda::corpus(text_field = "text")
# 
# # 2. Limpio el corpus y armo una matriz de términos
# # Abro un archivo con stopwords propias y modismos
# vector <- file.path('.', 'data', 'Materiales', 'stopes.csv') %>% 
#   read.csv(sep = ";", encoding = "utf-8", stringsAsFactors = FALSE) %>% 
#   transmute(stop = as.character(X0))
# 
# # Aplico la función dfm con los argumentos para limpiar el texto
# mydfm <- dfm(js_fav,
#              stem = FALSE,
#              tolower = TRUE,                                         # paso a minúscula todas las palabras
#              remove = c(stopwords("spanish"), vector),               # saco las palabras definidas en stop
#              remove_punct = TRUE,                                    # elimino puntuaciones
#              remove_numbers = TRUE,                                  # elimino números
#              verbose = TRUE)
# 
# # Me quedo sólo con las palabras
# palabras <- featnames(mydfm)
# 
# # Saco palabras con 1 y 2 caracteres
# palabras_tam_1 <- palabras[sapply(palabras, stringr::str_length) == 1]
# palabras_tam_2 <- palabras[sapply(palabras, stringr::str_length) == 2]
# 
# # uno el vector con otra/s palabra/s
# otraspalabras <- c("t.co",
#                    "https",
#                    "juan",
#                    "sartori",
#                    "juan sartori",
#                    "sartori",
#                    "@juansartoriuy",
#                    palabras_tam_1, 
#                    palabras_tam_2)          # todas las palabras a eliminar
# 
# # lo renuevo
# mydfm <- dfm(mydfm, remove = otraspalabras)
# 
# 
# # 3. Grafico las 20 principales palabras
# #creo un objeto con la 20 principales palabras 
# top_js_fav = data.frame(topfeatures(mydfm, 20))
# 
# 
# #defino las palabras como rownames
# top_js_fav$palabra = rownames(top_js_fav)
# 
# #grafico con ggplot los 20 términos principales 
# 
# plot_js_fav = top_js_fav[1:20, ] %>%
#   ggplot(aes(x = reorder(palabra, 
#                          topfeatures.mydfm..20.), 
#              y = topfeatures.mydfm..20., 
#              fill = palabra)) + 
#   geom_col(show.legend = FALSE) +
#   coord_flip() +
#   geom_text(aes(hjust = -0.1, 
#                 label = topfeatures.mydfm..20.)) +
#   theme_minimal() +
#   theme(axis.title.y = element_blank(),
#         axis.title.x = element_blank(),
#         axis.text = element_text(size = 15)) +
#   ggtitle("Palabras más frecuentes (n=20)")
